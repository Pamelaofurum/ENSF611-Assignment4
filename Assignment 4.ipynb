{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: Pamela Ofurum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (299, 12)\n",
      "X Type: <class 'pandas.core.frame.DataFrame'>\n",
      "y Shape: (299, 1)\n",
      "y Type: <class 'pandas.core.frame.DataFrame'>\n",
      "First 20 rows of the dataset:\n",
      "     age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "0   75.0        0                       582         0                 20   \n",
      "1   55.0        0                      7861         0                 38   \n",
      "2   65.0        0                       146         0                 20   \n",
      "3   50.0        1                       111         0                 20   \n",
      "4   65.0        1                       160         1                 20   \n",
      "5   90.0        1                        47         0                 40   \n",
      "6   75.0        1                       246         0                 15   \n",
      "7   60.0        1                       315         1                 60   \n",
      "8   65.0        0                       157         0                 65   \n",
      "9   80.0        1                       123         0                 35   \n",
      "10  75.0        1                        81         0                 38   \n",
      "11  62.0        0                       231         0                 25   \n",
      "12  45.0        1                       981         0                 30   \n",
      "13  50.0        1                       168         0                 38   \n",
      "14  49.0        1                        80         0                 30   \n",
      "15  82.0        1                       379         0                 50   \n",
      "16  87.0        1                       149         0                 38   \n",
      "17  45.0        0                       582         0                 14   \n",
      "18  70.0        1                       125         0                 25   \n",
      "19  48.0        1                       582         1                 55   \n",
      "\n",
      "    high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
      "0                     1  265000.00               1.9           130    1   \n",
      "1                     0  263358.03               1.1           136    1   \n",
      "2                     0  162000.00               1.3           129    1   \n",
      "3                     0  210000.00               1.9           137    1   \n",
      "4                     0  327000.00               2.7           116    0   \n",
      "5                     1  204000.00               2.1           132    1   \n",
      "6                     0  127000.00               1.2           137    1   \n",
      "7                     0  454000.00               1.1           131    1   \n",
      "8                     0  263358.03               1.5           138    0   \n",
      "9                     1  388000.00               9.4           133    1   \n",
      "10                    1  368000.00               4.0           131    1   \n",
      "11                    1  253000.00               0.9           140    1   \n",
      "12                    0  136000.00               1.1           137    1   \n",
      "13                    1  276000.00               1.1           137    1   \n",
      "14                    1  427000.00               1.0           138    0   \n",
      "15                    0   47000.00               1.3           136    1   \n",
      "16                    0  262000.00               0.9           140    1   \n",
      "17                    0  166000.00               0.8           127    1   \n",
      "18                    1  237000.00               1.0           140    0   \n",
      "19                    0   87000.00               1.9           121    0   \n",
      "\n",
      "    smoking  time  \n",
      "0         0     4  \n",
      "1         0     6  \n",
      "2         1     7  \n",
      "3         0     7  \n",
      "4         0     8  \n",
      "5         1     8  \n",
      "6         0    10  \n",
      "7         1    10  \n",
      "8         0    10  \n",
      "9         1    10  \n",
      "10        1    10  \n",
      "11        1    10  \n",
      "12        0    11  \n",
      "13        0    11  \n",
      "14        0    12  \n",
      "15        0    13  \n",
      "16        0    14  \n",
      "17        0    14  \n",
      "18        0    15  \n",
      "19        0    15  \n",
      "\n",
      "Information about the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      " 11  time                      299 non-null    int64  \n",
      "dtypes: float64(3), int64(9)\n",
      "memory usage: 28.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import dataset (1 mark)\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "  \n",
    "# fetch dataset \n",
    "heart_failure_clinical_records = fetch_ucirepo(id=519) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_failure_clinical_records.data.features \n",
    "y = heart_failure_clinical_records.data.targets \n",
    "  \n",
    "# metadata \n",
    "#print(heart_failure_clinical_records.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(heart_failure_clinical_records.variables) \n",
    "\n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"X Type:\", type(X))\n",
    "print(\"y Shape:\", y.shape)\n",
    "print(\"y Type:\", type(y))\n",
    " \n",
    "# Display the first 20 rows of the dataset\n",
    "print(\"First 20 rows of the dataset:\")\n",
    "print(X.head(20))\n",
    " \n",
    "# Display information about data types and missing values\n",
    "print(\"\\nInformation about the dataset:\")\n",
    "print(X.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "2. (1 mark) Why did you pick this particular dataset?\n",
    "3. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. What is the source of your dataset?\n",
    "The dataset was fetched from the UCI Machine Learning Repository (ucimlrepo). Specifically, it was obtained using the fetch_ucirepo function. Below is the Webiste URL;\n",
    "http://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records\n",
    "\n",
    "2. Why did you pick this particular dataset?\n",
    "I chose this particular dataset for analysis because of its rich and comprehensive collection of patient data related to heart failure and also because I am particularly interested in investigating causes of heart failure in patients as well as its treatment and prevention. I find the heart failure clinical records dataset particularly intriguing for analysis because it presents a wealth of detailed information on patients dealing with heart failure. Its comprehensive coverage of clinical variables, combined with insights into patient demographics and medical history, makes it a valuable resource for delving into the factors that influence heart failure. By leveraging this dataset, I hope to uncover meaningful patterns, identify potential risk factors, and contribute to the development of more effective strategies for preventing and treating heart failure. The dataset's reliability from a reputable source and well-documented structure further instills my confidence in its usability, making it an ideal foundation for my exploration into the complexities of cardiovascular health.\n",
    "\n",
    "\n",
    "3. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "Yes finding a particular dataset that matches my interest was quite challenging to me as I had to search extensively before I finally found the heart failure clinical records which sparked my interest for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afc244d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape before cleaning: (299, 12)\n",
      "X Type before cleaning: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "X Shape after cleaning: (283, 12)\n",
      "X Type after cleaning: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "\n",
    "# Fetch dataset\n",
    "heart_failure_clinical_records = fetch_ucirepo(id=519)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = heart_failure_clinical_records.data.features\n",
    "\n",
    "# Display the shape and type of X before cleaning\n",
    "print(\"X Shape before cleaning:\", X.shape)\n",
    "print(\"X Type before cleaning:\", type(X))\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "# Handling missing values using imputation\n",
    "imputer = SimpleImputer(strategy='mean')  # You can choose a different strategy if needed\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Removing duplicate rows\n",
    "X = X.drop_duplicates()\n",
    "\n",
    "# Handling outliers using z-score\n",
    "z_scores = stats.zscore(X)\n",
    "X = X[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Convert categorical variables to the appropriate type if needed\n",
    "# (Replace 'categorical_column' and 'category' with actual column name and desired data type)\n",
    "# X['categorical_column'] = X['categorical_column'].astype('category')\n",
    "\n",
    "# Scaling numerical features using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "# (Replace 'numeric_feature1' and 'numeric_feature2' with actual column names)\n",
    "# X[['numeric_feature1', 'numeric_feature2']] = scaler.fit_transform(X[['numeric_feature1', 'numeric_feature2']])\n",
    "\n",
    "# Display the shape and type of X after cleaning\n",
    "print(\"\\nX Shape after cleaning:\", X.shape)\n",
    "print(\"X Type after cleaning:\", type(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (299, 12)\n",
      "X Type: <class 'pandas.core.frame.DataFrame'>\n",
      "y Shape: (299, 1)\n",
      "y Type: <class 'pandas.core.frame.DataFrame'>\n",
      "First 20 rows of the dataset:\n",
      "     age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "0   75.0        0                       582         0                 20   \n",
      "1   55.0        0                      7861         0                 38   \n",
      "2   65.0        0                       146         0                 20   \n",
      "3   50.0        1                       111         0                 20   \n",
      "4   65.0        1                       160         1                 20   \n",
      "5   90.0        1                        47         0                 40   \n",
      "6   75.0        1                       246         0                 15   \n",
      "7   60.0        1                       315         1                 60   \n",
      "8   65.0        0                       157         0                 65   \n",
      "9   80.0        1                       123         0                 35   \n",
      "10  75.0        1                        81         0                 38   \n",
      "11  62.0        0                       231         0                 25   \n",
      "12  45.0        1                       981         0                 30   \n",
      "13  50.0        1                       168         0                 38   \n",
      "14  49.0        1                        80         0                 30   \n",
      "15  82.0        1                       379         0                 50   \n",
      "16  87.0        1                       149         0                 38   \n",
      "17  45.0        0                       582         0                 14   \n",
      "18  70.0        1                       125         0                 25   \n",
      "19  48.0        1                       582         1                 55   \n",
      "\n",
      "    high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
      "0                     1  265000.00               1.9           130    1   \n",
      "1                     0  263358.03               1.1           136    1   \n",
      "2                     0  162000.00               1.3           129    1   \n",
      "3                     0  210000.00               1.9           137    1   \n",
      "4                     0  327000.00               2.7           116    0   \n",
      "5                     1  204000.00               2.1           132    1   \n",
      "6                     0  127000.00               1.2           137    1   \n",
      "7                     0  454000.00               1.1           131    1   \n",
      "8                     0  263358.03               1.5           138    0   \n",
      "9                     1  388000.00               9.4           133    1   \n",
      "10                    1  368000.00               4.0           131    1   \n",
      "11                    1  253000.00               0.9           140    1   \n",
      "12                    0  136000.00               1.1           137    1   \n",
      "13                    1  276000.00               1.1           137    1   \n",
      "14                    1  427000.00               1.0           138    0   \n",
      "15                    0   47000.00               1.3           136    1   \n",
      "16                    0  262000.00               0.9           140    1   \n",
      "17                    0  166000.00               0.8           127    1   \n",
      "18                    1  237000.00               1.0           140    0   \n",
      "19                    0   87000.00               1.9           121    0   \n",
      "\n",
      "    smoking  time  \n",
      "0         0     4  \n",
      "1         0     6  \n",
      "2         1     7  \n",
      "3         0     7  \n",
      "4         0     8  \n",
      "5         1     8  \n",
      "6         0    10  \n",
      "7         1    10  \n",
      "8         0    10  \n",
      "9         1    10  \n",
      "10        1    10  \n",
      "11        1    10  \n",
      "12        0    11  \n",
      "13        0    11  \n",
      "14        0    12  \n",
      "15        0    13  \n",
      "16        0    14  \n",
      "17        0    14  \n",
      "18        0    15  \n",
      "19        0    15  \n",
      "\n",
      "Information about the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      " 11  time                      299 non-null    int64  \n",
      "dtypes: float64(3), int64(9)\n",
      "memory usage: 28.2 KB\n",
      "None\n",
      "\n",
      "X_train_preprocessed Shape: (239, 4)\n",
      "X_train_preprocessed Type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed\n",
    "\n",
    "# Fetch dataset\n",
    "heart_failure_clinical_records = fetch_ucirepo(id=519)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = heart_failure_clinical_records.data.features\n",
    "y = heart_failure_clinical_records.data.targets\n",
    "\n",
    " # Ensure the number of samples in X and y is consistent\n",
    "if X.shape[0] != y.shape[0]:\n",
    "     raise ValueError(\"Number of samples in X and y are inconsistent.\")\n",
    "\n",
    "# Display the shape and type of X and y\n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"X Type:\", type(X))\n",
    "print(\"y Shape:\", y.shape)\n",
    "print(\"y Type:\", type(y))\n",
    "\n",
    "# Display the first 20 rows of the dataset\n",
    "print(\"First 20 rows of the dataset:\")\n",
    "print(X.head(20))\n",
    "\n",
    "# Display information about data types and missing values\n",
    "print(\"\\nInformation about the dataset:\")\n",
    "print(X.info())\n",
    "\n",
    "# Split the data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define preprocessing steps using ColumnTransformer\n",
    "# Replace with actual column names from your dataset\n",
    "numeric_features = ['age', 'serum_creatinine']\n",
    "categorical_features = ['sex']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n",
    "    ('scaler', StandardScaler())  # Standardize numeric features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Replace missing values with most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to training and testing sets\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Display the shape and type of preprocessed X_train\n",
    "print(\"\\nX_train_preprocessed Shape:\", X_train_preprocessed.shape)\n",
    "print(\"X_train_preprocessed Type:\", type(X_train_preprocessed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "\n",
    "Yes there were missing values in the original dataset, and these missing values were addressed during the data cleaning process\n",
    "inorder to maintain data integrity, avoid bias, ensure overall data quality to achieve more accurate insights which is crucial for drawing meaningful conclusions.\n",
    "\n",
    "Below is a summary of the cleaning process and how the missing values were replaced;\n",
    "\n",
    "Initial State:\n",
    "X Shape before cleaning: (299, 12)\n",
    "X Type before cleaning: <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "Handling Missing Values:\n",
    "The code X = X.dropna() was used to drop rows with missing values.\n",
    "\n",
    "Result After Cleaning:\n",
    "X Shape after cleaning: (283, 12)\n",
    "X Type after cleaning: <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "The initial dataset had dimensions of 299 rows and 12 columns (X Shape before cleaning: (299, 12)), indicating that it contained missing values. During the data cleaning process, a common approach was employed by using the dropna() method to remove rows with missing values. This resulted in a refined dataset with dimensions of 283 rows and 12 columns (X Shape after cleaning: (283, 12)).\n",
    "\n",
    "2. What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "Type of Data:\n",
    "\n",
    "The dataset consists of clinical records with 299 entries and 12 columns.\n",
    "The columns include features such as age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, platelets, serum_creatinine, serum_sodium, sex, smoking, and time.\n",
    "The data types include both numerical (float64 and int64) and categorical (int64) features.\n",
    "\n",
    "Preprocessing Methods Based on Data Types:\n",
    "\n",
    "Numerical Features:\n",
    "\n",
    "Imputation: Missing values in numerical features could be imputed using methods such as mean imputation (SimpleImputer(strategy='mean')) to replace missing values with the mean of each column.\n",
    "\n",
    "Scaling: Numerical features can be scaled to a standard range using techniques like Min-Max scaling or StandardScaler. In this case, StandardScaler is applied (StandardScaler()).\n",
    "\n",
    "Categorical Features:\n",
    "\n",
    "Imputation: For categorical features, missing values can be replaced with the most frequent value using SimpleImputer(strategy='most_frequent').\n",
    "\n",
    "Encoding: Categorical features are one-hot encoded using OneHotEncoder(handle_unknown='ignore') to represent them as binary vectors.\n",
    "\n",
    "In summary, the preprocessing methods applied include imputing missing values, scaling numerical features, and encoding categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5558a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Linear Model: {'classifier__C': 1, 'classifier__penalty': 'l1'}\n",
      "Best Hyperparameters for Non-linear Model 1: {'classifier__max_depth': None, 'classifier__n_estimators': 150}\n",
      "Best Hyperparameters for Non-linear Model 2: {'classifier__C': 10, 'classifier__kernel': 'rbf'}\n",
      "\n",
      "Accuracy on Test Set (Linear Model): 0.7\n",
      "Classification Report (Linear Model):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.80        41\n",
      "           1       0.56      0.26      0.36        19\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.58      0.58        60\n",
      "weighted avg       0.67      0.70      0.66        60\n",
      "\n",
      "\n",
      "Accuracy on Test Set (Non-linear Model 1): 0.7\n",
      "Classification Report (Non-linear Model 1):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.80        41\n",
      "           1       0.54      0.37      0.44        19\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.61      0.62        60\n",
      "weighted avg       0.68      0.70      0.68        60\n",
      "\n",
      "\n",
      "Accuracy on Test Set (Non-linear Model 2): 0.6833333333333333\n",
      "Classification Report (Non-linear Model 2):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.90      0.80        41\n",
      "           1       0.50      0.21      0.30        19\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.61      0.56      0.55        60\n",
      "weighted avg       0.64      0.68      0.64        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "heart_failure_clinical_records = fetch_ucirepo(id=519)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = heart_failure_clinical_records.data.features\n",
    "y = heart_failure_clinical_records.data.targets.values.ravel()\n",
    "\n",
    "# Ensure the number of samples in X and y is consistent\n",
    "if X.shape[0] != y.shape[0]:\n",
    "    raise ValueError(\"Number of samples in X and y are inconsistent.\")\n",
    "\n",
    "# Split the data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define preprocessing steps using ColumnTransformer\n",
    "numeric_features = ['age', 'serum_creatinine']\n",
    "categorical_features = ['sex']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n",
    "    ('scaler', StandardScaler())  # Standardize numeric features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Replace missing values with most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define machine learning models\n",
    "# Define machine learning models\n",
    "linear_model = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', penalty='l1')\n",
    "nonlinear_model1 = RandomForestClassifier(random_state=42)\n",
    "nonlinear_model2 = SVC(random_state=42)\n",
    "\n",
    "# Define the pipeline for each model\n",
    "linear_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', linear_model)\n",
    "])\n",
    "\n",
    "nonlinear_pipeline1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', nonlinear_model1)\n",
    "])\n",
    "\n",
    "nonlinear_pipeline2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', nonlinear_model2)\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search for each model\n",
    "param_grid_linear = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "param_grid_nonlinear1 = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "param_grid_nonlinear2 = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "grid_search_linear = GridSearchCV(linear_pipeline, param_grid_linear, cv=5, scoring='accuracy', error_score='raise')\n",
    "grid_search_linear.fit(X_train, y_train)\n",
    "\n",
    "grid_search_nonlinear1 = GridSearchCV(nonlinear_pipeline1, param_grid_nonlinear1, cv=5, scoring='accuracy', error_score='raise')\n",
    "grid_search_nonlinear1.fit(X_train, y_train)\n",
    "\n",
    "grid_search_nonlinear2 = GridSearchCV(nonlinear_pipeline2, param_grid_nonlinear2, cv=5, scoring='accuracy', error_score='raise')\n",
    "grid_search_nonlinear2.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters for each model\n",
    "print(\"Best Hyperparameters for Linear Model:\", grid_search_linear.best_params_)\n",
    "print(\"Best Hyperparameters for Non-linear Model 1:\", grid_search_nonlinear1.best_params_)\n",
    "print(\"Best Hyperparameters for Non-linear Model 2:\", grid_search_nonlinear2.best_params_)\n",
    "\n",
    "# Evaluate each model on the test set\n",
    "y_pred_linear = grid_search_linear.predict(X_test)\n",
    "y_pred_nonlinear1 = grid_search_nonlinear1.predict(X_test)\n",
    "y_pred_nonlinear2 = grid_search_nonlinear2.predict(X_test)\n",
    "\n",
    "# Display evaluation metrics for each model\n",
    "print(\"\\nAccuracy on Test Set (Linear Model):\", accuracy_score(y_test, y_pred_linear))\n",
    "print(\"Classification Report (Linear Model):\\n\", classification_report(y_test, y_pred_linear))\n",
    "\n",
    "print(\"\\nAccuracy on Test Set (Non-linear Model 1):\", accuracy_score(y_test, y_pred_nonlinear1))\n",
    "print(\"Classification Report (Non-linear Model 1):\\n\", classification_report(y_test, y_pred_nonlinear1))\n",
    "\n",
    "print(\"\\nAccuracy on Test Set (Non-linear Model 2):\", accuracy_score(y_test, y_pred_nonlinear2))\n",
    "print(\"Classification Report (Non-linear Model 2):\\n\", classification_report(y_test, y_pred_nonlinear2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. Do you need regression or classification models for your dataset?\n",
    " Classification models.\n",
    "\n",
    "2. Which models did you select for testing and why?\n",
    "I selected three different supervised learning models for testing as seen below and the reasons are also explained below;\n",
    "\n",
    "Logistic Regression (Linear Model): The reason is selecting this model is because Logistic Regression is a commonly used linear model for binary classification problems and it's a simple and interpretable model that works well when the relationship between features and the log-odds of the response is approximately linear.\n",
    "\n",
    "Random Forest Classifier (Non-linear Model 1): The reason for selecting this model is because Random Forest is an ensemble learning method that can capture non-linear relationships in the data and handle complex interactions between features. It is robust and handles non-linearity well, and provides good performance without extensive hyperparameter tuning.\n",
    "\n",
    "Support Vector Classifier (Non-linear Model 2): The reason for selecting this model is because Support Vector Classifier (SVC) with a non-linear kernel (e.g., radial basis function - rbf) is capable of learning complex decision boundaries and it is effective in high-dimensional spaces and is also suitable for non-linear classification problems.\n",
    "\n",
    "My selection of these models reflects a strategic mix of linear and non-linear approaches, providing a well-rounded exploration of the unique characteristics within the dataset I'm working with. I carefully considered each model based on its appropriateness for classification tasks and its ability to uncover diverse patterns in the data. My objective is to conduct a thorough performance comparison, which aims to identify the model that strikes the optimal balance between accuracy and the ability to generalize to new data.\n",
    "\n",
    "3. Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "Logistic Regression (Linear Model):\n",
    "Accuracy on Test Set: 0.7\n",
    "Classification Report:\n",
    "      precision    recall  f1-score   support\n",
    " 0     0.73      0.90      0.80        41\n",
    " 1     0.56      0.26      0.36        19\n",
    "\n",
    "Random Forest Classifier (Non-linear Model 1):\n",
    "Accuracy on Test Set: 0.7\n",
    "Classification Report:\n",
    "    precision    recall  f1-score   support\n",
    "0       0.74      0.85      0.80        41\n",
    "1       0.54      0.37      0.44        19\n",
    "\n",
    "Support Vector Classifier (Non-linear Model 2):\n",
    "Accuracy on Test Set: 0.6833333333333333\n",
    "Classification Report:\n",
    "    precision    recall  f1-score   support\n",
    "0       0.71      0.90      0.80        41\n",
    "1       0.50      0.21      0.30        19\n",
    "\n",
    "Based on the results above, the Logistic Regression model and Random Forest Classifier achieved the highest accuracy among the three models while the Support Vector Classifier (SVC) had slightly lower accuracy compared to the other two models.\n",
    "\n",
    "Random Forest Classifier Model worked the best, as it achieved a high accuracy on the test set (0.7) and highest precision (0.74) compared to the other models.\n",
    "\n",
    "Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "Yes, it absolutely makes sense based on the theory discussed in the course and the context of my dataset (Heart Failure Clinical Records).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Linear Model: {'classifier__C': 1, 'classifier__penalty': 'l1'}\n",
      "Best Hyperparameters for Non-linear Model 1: {'classifier__max_depth': None, 'classifier__n_estimators': 150}\n",
      "Best Hyperparameters for Non-linear Model 2: {'classifier__C': 10, 'classifier__kernel': 'rbf'}\n",
      "\n",
      "Accuracy on Test Set (Linear Model): 0.7\n",
      "Classification Report (Linear Model):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.80        41\n",
      "           1       0.56      0.26      0.36        19\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.58      0.58        60\n",
      "weighted avg       0.67      0.70      0.66        60\n",
      "\n",
      "\n",
      "Accuracy on Test Set (Non-linear Model 1): 0.7\n",
      "Classification Report (Non-linear Model 1):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.80        41\n",
      "           1       0.54      0.37      0.44        19\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.61      0.62        60\n",
      "weighted avg       0.68      0.70      0.68        60\n",
      "\n",
      "\n",
      "Accuracy on Test Set (Non-linear Model 2): 0.6833333333333333\n",
      "Classification Report (Non-linear Model 2):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.90      0.80        41\n",
      "           1       0.50      0.21      0.30        19\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.61      0.56      0.55        60\n",
      "weighted avg       0.64      0.68      0.64        60\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIOCAYAAACPj11ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABLEklEQVR4nO3de3yP9f/H8eeHnTeGYc7bkFNOmXMhyZxDiVKGkaSaUErKWVPiqxTqywyJkcM3EpbDEPlGlsPUtyJzGELmEMP2/v3h5vPzsc21D+Oz8rjfbp/b9/t5X+/rul7Xtetz9Xm6ruv9sRljjAAAAAAAWcrj6gIAAAAAILcjOAEAAACABYITAAAAAFggOAEAAACABYITAAAAAFggOAEAAACABYITAAAAAFggOAEAAACABYITAAAAAFggOAG4oz788EPZbDZVrVrV1aXgBsHBwbLZbJavmJiYHFnfO++8o6VLl2a7/8mTJzVkyBBVqVJFvr6+8vf3V6VKldStWzft3LnT6fUfOXJEI0aMUEJCQrbnWbNmjWrXri1fX1/ZbDan6nfW77//7rDf3d3dFRAQoDp16mjAgAHas2dPhnnWr18vm82m9evXO7RPnjxZ5cuXl4eHh2w2m06fPi1Jeuutt1SmTBm5ubmpQIECd2xbbldiYqJGjBih33//PVv9Y2Ji7Pvtxn0hScYYlS9fXjabTQ8//HCO1mqz2TRixAin57v2986pzxeAO8/N1QUA+GeLjo6WJO3Zs0dbt25VvXr1XFwRrlmyZIlSU1Pt76dPn64ZM2Zo5cqV8vf3t7eXK1cuR9b3zjvvqFOnTurQoYNl33Pnzql+/fo6d+6cXnvtNdWoUUMXLlzQ//73Py1evFgJCQmqXr26U+s/cuSIRo4cqeDgYNWsWdOyvzFGnTt3VoUKFfTll1/K19dXFStWdGqdt+Lll19W165dlZ6ertOnT2vHjh2Kjo7W5MmTFRUVpddee83et1atWtqyZYuqVKlib0tISFBkZKR69+6t7t27y83NTfny5dN//vMfjR07VkOHDlWrVq3k6el5x7flViUmJmrkyJF6+OGHFRwcnO358uXLpxkzZmQIR/Hx8frtt9+UL1++nC0UwD2F4ATgjtm2bZt+/PFHtWnTRl999ZVmzJiRa4PTX3/9JR8fH1eXcVc98MADDu9XrlwpSQoNDVXhwoVdUZLdwoUL9euvv2rt2rVq2rSpw7SBAwcqPT39jtdw5MgRnTp1Sh07dlSzZs1yZJkXLlyQl5eXbDZbln3KlCmj+vXr29+3bt1aAwcO1OOPP67BgweratWqatWqlSQpf/78Dn0l2a9MPffcc6pbt669fffu3ZKkyMhIFS1aNEe2J7d9brp06aK5c+fq448/Vv78+e3tM2bMUIMGDXTmzBkXVgfg745b9QDcMTNmzJAkjRs3Tg0bNtT8+fP1119/Zeh3+PBh9enTR6VLl5aHh4dKlCihTp066dixY/Y+p0+f1qBBg1S2bFl5enqqaNGiat26tX766SdJWd+ylNntMD169JCfn5927dqlsLAw5cuXz/7FOC4uTu3bt1epUqXk5eWl8uXL6/nnn9eJEycy1P3TTz/p6aefVmBgoDw9PVWmTBmFh4crNTVVv//+u9zc3BQVFZVhvg0bNshms2nhwoU33X9JSUl69tlnVbRoUXl6eqpy5cqaMGGCQ2i4tn3vv/++Jk6cqJCQEPn5+alBgwb67rvvbrr87DDGaMqUKapZs6a8vb1VsGBBderUSfv27XPot2PHDrVt29Zea4kSJdSmTRsdOnRI0tXbmc6fP69Zs2bZb6m62S1TJ0+elCQVL1480+l58jj+5+uXX35R165dHfbVxx9/bJ++fv161alTR5LUs2dPew1Z3WI1YsQIlSpVSpL0+uuvy2azOVz52LRpk5o1a6Z8+fLJx8dHDRs21FdffeWwjGu3j61evVoREREqUqSIfHx8HK7yZZe3t7dmzJghd3d3jR8/3mG7rj/uH374YT377LOSpHr16slms6lHjx4KDg7WW2+9JUkKDAzMsO2xsbFq0KCBfH195efnpxYtWmjHjh0ONdzsc3Pp0iWNGTNGlSpVkqenp4oUKaKePXvqjz/+cFhGcHCw2rZtq5UrV6pWrVry9vZWpUqV7Femr+23J598UpLUtGlTp24ZffrppyVJ8+bNs7elpKRo0aJFioiIyHSeU6dOqV+/fipZsqQ8PDxUtmxZDR06NMPf6cyZM3ruuecUEBAgPz8/tWzZUv/73/8yXabV8ZiVP/74w34uvLYfH3zwQX3zzTeW8wK4CwwA3AF//fWX8ff3N3Xq1DHGGDN9+nQjycTExDj0O3TokClevLgpXLiwmThxovnmm29MbGysiYiIMHv37jXGGHPmzBlz//33G19fXzNq1CizatUqs2jRItO/f3+zdu1aY4wx69atM5LMunXrHJa/f/9+I8nMnDnT3ta9e3fj7u5ugoODTVRUlFmzZo1ZtWqVMcaYqVOnmqioKPPll1+a+Ph4M2vWLFOjRg1TsWJFc+nSJfsyEhISjJ+fnwkODjbTpk0za9asMZ999pnp3LmzOXPmjDHGmI4dO5oyZcqYK1euONT05JNPmhIlSpjLly9nuf+OHz9uSpYsaYoUKWKmTZtmVq5caV566SUjybzwwgsZti84ONi0bNnSLF261CxdutRUq1bNFCxY0Jw+fTo7fy5jjDHDhw83kswff/xhb3vuueeMu7u7GTRokFm5cqX5/PPPTaVKlUxgYKA5evSoMcaYc+fOmYCAAFO7dm2zYMECEx8fb2JjY03fvn1NYmKiMcaYLVu2GG9vb9O6dWuzZcsWs2XLFrNnz54sa9m0aZORZOrUqWOWLFliTpw4kWXfPXv2GH9/f1OtWjUze/Zss3r1ajNo0CCTJ08eM2LECGOMMSkpKWbmzJlGknnrrbfsNRw8eDDTZR48eNAsXrzYSDIvv/yy2bJli/nhhx+MMcasX7/euLu7m9DQUBMbG2uWLl1qwsLCjM1mM/Pnz7cv49r6SpYsafr06WO+/vpr88UXX2Q4Hq659rccP358lttav3594+npaT92bjzu9+zZY9566y37Mb9lyxbz66+/mh9++MH06tXLSDIrV6502PaxY8cam81mIiIizPLly83ixYtNgwYNjK+vr8PfKKvPTVpammnZsqXx9fU1I0eONHFxcWb69OmmZMmSpkqVKuavv/6yLyMoKMiUKlXKVKlSxcyePdusWrXKPPnkk0aSiY+PN8ZcPfbfeecdI8l8/PHH9r/V8ePHs9wv1/b1999/b7p162bq1q1rnzZ16lTj6+trP480adLEPu3ChQumevXqxtfX17z//vtm9erV5u233zZubm6mdevW9n7p6emmadOmxtPT04wdO9asXr3aDB8+3JQtW9ZIMsOHD7f3zc7xeP3f+/pzU4sWLUyRIkXMp59+atavX2+WLl1qhg0b5nBcAXAdghOAO2L27NlGkpk2bZoxxpizZ88aPz8/06hRI4d+ERERxt3d3f4FOzOjRo0ykkxcXFyWfZwNTpJMdHT0TbchPT3dXL582Rw4cMBIMv/5z3/s0x555BFToECBm36Zu1bTkiVL7G2HDx82bm5uZuTIkTdd9xtvvGEkma1btzq0v/DCC8Zms5mff/7ZYfuqVavm8IX8v//9r5Fk5s2bd9P1XO/G4LRlyxYjyUyYMMGh38GDB423t7cZPHiwMcaYbdu2GUlm6dKlN12+r6+v6d69e7brGTVqlPHw8DCSjCQTEhJi+vbta3788UeHfi1atDClSpUyKSkpDu0vvfSS8fLyMqdOnTLGGPP9999nOBZuJqsgU79+fVO0aFFz9uxZe9uVK1dM1apVTalSpUx6erox5v+/zIeHh9/W+q7XpUsXI8kcO3bMGJP5cX99iLheZsE4KSnJuLm5mZdfftmh79mzZ02xYsVM586d7W1ZfW7mzZtnJJlFixY5tF/b31OmTLG3BQUFGS8vL3PgwAF724ULF0yhQoXM888/b29buHBhpp/nrFy/zdf2ye7du40xxtSpU8f06NHDGGMyBKdp06YZSWbBggUOy3v33XeNJLN69WpjjDFff/21kWQ++OADh35jx47NEJyyezxmdm7y8/Mzr7zySra2GcDdx616AO6IGTNmyNvbW0899ZQkyc/PT08++aQ2btyoX375xd7v66+/VtOmTVW5cuUsl/X111+rQoUKevTRR3O0xieeeCJD2/Hjx9W3b1+VLl1abm5ucnd3V1BQkCRp7969kq4+1xEfH6/OnTurSJEiWS7/4YcfVo0aNRxu0Zk2bZpsNpv69Olz09rWrl2rKlWqODyjIl29XcoYo7Vr1zq0t2nTRnnz5rW/vzZwwoEDB266nptZvny5bDabnn32WV25csX+KlasmGrUqGG/Pax8+fIqWLCgXn/9dU2bNk2JiYm3vM7rvf3220pKSlJ0dLSef/55+fn5adq0aQoNDbXfinXx4kWtWbNGHTt2lI+Pj0OdrVu31sWLF3PklsVrzp8/r61bt6pTp07y8/Ozt+fNm1fdunXToUOH9PPPPzvMk9lxdquMMTm2LElatWqVrly5ovDwcId95+XlpSZNmmQ6Qt2N27N8+XIVKFBA7dq1c1hGzZo1VaxYsQzLqFmzpsqUKWN/7+XlpQoVKtzWsXq9Jk2aqFy5coqOjtauXbv0/fffZ3mb3tq1a+Xr66tOnTo5tPfo0UPS1VEVJWndunWSpGeeecahX9euXR3e3+7xWLduXcXExGjMmDH67rvvdPnyZae2HcCdRXACkON+/fVXbdiwQW3atJExRqdPn9bp06ftX06uf57hjz/+sD9LkpXs9HGWj4+Pw8PjkpSenq6wsDAtXrxYgwcP1po1a/Tf//7X/kXnwoULkqQ///xTaWlp2aopMjJSa9as0c8//6zLly/r3//+tzp16qRixYrddL6TJ09m+nxPiRIl7NOvFxAQ4PD+2ohp12q+FceOHZMxRoGBgXJ3d3d4fffdd/bnvvz9/RUfH6+aNWvqzTff1P33368SJUpo+PDht/3FLzAwUD179tS0adO0c+dOxcfHy8PDQ/3795d0dT9cuXJFkydPzlBj69atJSnT59Nu1Z9//iljjFN/m6ye07oVBw4ckKenpwoVKpQjy7v2HGGdOnUy7L/Y2NgM+y6zz82xY8d0+vRpeXh4ZFjG0aNHMyzjxmNVunq83s6xej2bzaaePXvqs88+07Rp01ShQgU1atQo074nT55UsWLFMgzWUbRoUbm5udn/lidPnpSbm1uG2m/8HN/u8RgbG6vu3btr+vTpatCggQoVKqTw8HAdPXrU6f0AIOcxqh6AHBcdHS1jjL744gt98cUXGabPmjVLY8aMUd68eVWkSBH7AAJZyU4fLy8vScrwQHdWX1IyG9Vs9+7d+vHHHxUTE6Pu3bvb23/99VeHfoUKFVLevHkta5Ku/ov066+/ro8//lj169fX0aNH9eKLL1rOFxAQoOTk5AztR44ckaS7Mupd4cKFZbPZtHHjxkyHrr6+rVq1apo/f76MMdq5c6diYmI0atQoeXt764033sixmho3bqywsDAtXbpUx48fV8GCBe1Xe7LaryEhITm2/oIFCypPnjxO/W1uNoKeMw4fPqzt27erSZMmcnPLmf98X6v1iy++sF9ZvZnMtqVw4cIKCAiwj8p4I1cMAd6jRw8NGzZM06ZN09ixY7PsFxAQoK1bt8oY47Btx48f15UrV+z7JyAgQFeuXNHJkycdwtONgeZ2j8fChQtr0qRJmjRpkpKSkvTll1/qjTfe0PHjx7PcvwDuHoITgByVlpamWbNmqVy5cpo+fXqG6cuXL9eECRP09ddfq23btmrVqpXmzJmjn3/+OcvfyGnVqpWGDRumtWvX6pFHHsm0z7URz3bu3KkWLVrY27/88sts137ti9ONIeGTTz5xeO/t7a0mTZpo4cKFGjt27E1DjJeXl/r06aOPPvpImzdvVs2aNfXggw9a1tKsWTNFRUXphx9+UK1atezts2fPls1myzBE953Qtm1bjRs3TocPH1bnzp2zNY/NZlONGjX0r3/9SzExMfrhhx/s05y5qnDs2DEVKVIkw+h5aWlp+uWXX+Tj46MCBQrIw8NDTZs21Y4dO1S9enV5eHhkucycuArn6+urevXqafHixXr//ffl7e0t6erVys8++0ylSpVShQoVbnn5Wblw4YJ69+6tK1euaPDgwTm23BYtWsjNzU2//fbbLd9S2LZtW82fP19paWk59nMDt/u3KlmypF577TX99NNPDv8IcqNmzZppwYIFWrp0qTp27Ghvnz17tn26dHV0v/fee09z585VZGSkvd/nn3/usDwfH59sH49WypQpo5deeklr1qzRt99+e8vLAZBzCE4ActTXX3+tI0eO6N133810uOmqVavqo48+0owZM9S2bVuNGjVKX3/9tRo3bqw333xT1apV0+nTp7Vy5UoNHDhQlSpV0iuvvKLY2Fi1b99eb7zxhurWrasLFy4oPj5ebdu2VdOmTVWsWDE9+uijioqKUsGCBRUUFKQ1a9Zo8eLF2a69UqVKKleunN544w0ZY1SoUCEtW7ZMcXFxGfpOnDhRDz30kOrVq6c33nhD5cuX17Fjx/Tll1/qk08+cfhX9n79+um9997T9u3bMw2TmRkwYIBmz56tNm3aaNSoUQoKCtJXX32lKVOm6IUXXrgjX85v9OCDD6pPnz7q2bOntm3bpsaNG8vX11fJycnatGmTqlWrphdeeEHLly/XlClT1KFDB5UtW1bGGC1evFinT59W8+bN7curVq2a1q9fr2XLlql48eLKly9flmF5zpw5+uSTT9S1a1fVqVNH/v7+OnTokKZPn649e/Zo2LBh9i+lH3zwgR566CE1atRIL7zwgoKDg3X27Fn9+uuvWrZsmf15sHLlysnb21tz585V5cqV5efnpxIlSthvscuuqKgoNW/eXE2bNtWrr74qDw8PTZkyRbt379a8efNu+wpTUlKSvvvuO6WnpyslJcX+A7gHDhzQhAkTFBYWdlvLv15wcLBGjRqloUOHat++fWrZsqUKFiyoY8eO6b///a98fX01cuTImy7jqaee0ty5c9W6dWv1799fdevWlbu7uw4dOqR169apffv2DqEkO6pWrSpJ+vTTT5UvXz55eXkpJCQk09v8sjJu3DjLPuHh4fr444/VvXt3/f7776pWrZo2bdqkd955R61bt7Y/VxkWFqbGjRtr8ODBOn/+vGrXrq1vv/1Wc+bMybDM7B6PN0pJSVHTpk3VtWtXVapUSfny5dP333+vlStX6vHHH8/2dgO4g1w1KgWAf6YOHToYDw+Pm44299RTTxk3Nzf7cNYHDx40ERERplixYsbd3d2UKFHCdO7c2T5ymDHG/Pnnn6Z///6mTJkyxt3d3RQtWtS0adPG/PTTT/Y+ycnJplOnTqZQoULG39/fPPvss/YR324cVc/X1zfT2hITE03z5s1Nvnz5TMGCBc2TTz5pkpKSMoycda3vk08+aQICAoyHh4cpU6aM6dGjh7l48WKG5T788MOmUKFCDkMzWzlw4IDp2rWrCQgIMO7u7qZixYpm/PjxJi0tzd7nZiOxZVbzzWQ26poxxkRHR5t69eoZX19f4+3tbcqVK2fCw8PNtm3bjDHG/PTTT+bpp5825cqVM97e3sbf39/UrVs3w9DzCQkJ5sEHHzQ+Pj5GksPoZjdKTEw0gwYNMrVr1zZFihQxbm5upmDBgqZJkyZmzpw5Gfrv37/fREREmJIlSxp3d3dTpEgR07BhQzNmzBiHfvPmzTOVKlUy7u7ulvvnZvt248aN5pFHHrHvk/r165tly5Y59MlqdDur9V175c2b1xQsWNCEhoaaV155JdPh2293VL1rli5dapo2bWry589vPD09TVBQkOnUqZP55ptv7H1u9rm5fPmyef/9902NGjWMl5eX8fPzM5UqVTLPP/+8+eWXX+z9goKCTJs2bTLM36RJkwzHw6RJk0xISIjJmzev5WiI2d3XN46qZ4wxJ0+eNH379jXFixc3bm5uJigoyAwZMiTD5/j06dMmIiLCFChQwPj4+JjmzZubn376KdPjKDvH442j6l28eNH07dvXVK9e3eTPn994e3ubihUrmuHDh5vz58/fdLsA3B02Y3J4iB4AgIPjx48rKChIL7/8st577z1XlwMAAG4Bt+oBwB1y6NAh7du3T+PHj1eePHnsI8EBAIC/H4YjB4A7ZPr06Xr44Ye1Z88ezZ07VyVLlnR1SQAA4BZxqx4AAAAAWHDpFacNGzaoXbt2KlGihGw2m5YuXWo5T3x8vEJDQ+Xl5aWyZctq2rRpd75QAAAAAPc0lwan8+fPq0aNGvroo4+y1X///v1q3bq1GjVqpB07dujNN99UZGSkFi1adIcrBQAAAHAvyzW36tlsNi1ZskQdOnTIss/rr7+uL7/8Unv37rW39e3bVz/++KO2bNlyF6oEAAAAcC/6W42qt2XLlgw//NeiRQvNmDFDly9flru7e4Z5UlNTlZqaan+fnp6uU6dOKSAg4LZ/pBAAAADA35cxRmfPnlWJEiWUJ8/Nb8b7WwWno0ePKjAw0KEtMDBQV65c0YkTJ1S8ePEM80RFRVn+6jkAAACAe9fBgwdVqlSpm/b5WwUnSRmuEl270zCrq0dDhgzRwIED7e9TUlJUpkwZHTx4UPnz579zhQIAAADI1c6cOaPSpUsrX758ln3/VsGpWLFiOnr0qEPb8ePH5ebmpoCAgEzn8fT0lKenZ4b2/PnzE5wAAAAAZOsRnr/VD+A2aNBAcXFxDm2rV69W7dq1M32+CQAAAABygkuD07lz55SQkKCEhARJV4cbT0hIUFJSkqSrt9mFh4fb+/ft21cHDhzQwIEDtXfvXkVHR2vGjBl69dVXXVE+AAAAgHuES2/V27Ztm5o2bWp/f+1ZpO7duysmJkbJycn2ECVJISEhWrFihQYMGKCPP/5YJUqU0IcffqgnnnjirtcOAAAA4N6Ra37H6W45c+aM/P39lZKSwjNOAAAAwD3MmWzwt3rGCQAAAABcgeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABgweXBacqUKQoJCZGXl5dCQ0O1cePGm/afO3euatSoIR8fHxUvXlw9e/bUyZMn71K1AAAAAO5FLg1OsbGxeuWVVzR06FDt2LFDjRo1UqtWrZSUlJRp/02bNik8PFy9evXSnj17tHDhQn3//ffq3bv3Xa4cAAAAwL3EpcFp4sSJ6tWrl3r37q3KlStr0qRJKl26tKZOnZpp/++++07BwcGKjIxUSEiIHnroIT3//PPatm3bXa4cAAAAwL3EZcHp0qVL2r59u8LCwhzaw8LCtHnz5kznadiwoQ4dOqQVK1bIGKNjx47piy++UJs2be5GyQAAAADuUS4LTidOnFBaWpoCAwMd2gMDA3X06NFM52nYsKHmzp2rLl26yMPDQ8WKFVOBAgU0efLkLNeTmpqqM2fOOLwAAAAAwBkuHxzCZrM5vDfGZGi7JjExUZGRkRo2bJi2b9+ulStXav/+/erbt2+Wy4+KipK/v7/9Vbp06RytHwAAAMA/n80YY1yx4kuXLsnHx0cLFy5Ux44d7e39+/dXQkKC4uPjM8zTrVs3Xbx4UQsXLrS3bdq0SY0aNdKRI0dUvHjxDPOkpqYqNTXV/v7MmTMqXbq0UlJSlD9//hzeKgAAAAB/F2fOnJG/v3+2soHLrjh5eHgoNDRUcXFxDu1xcXFq2LBhpvP89ddfypPHseS8efNKunqlKjOenp7Knz+/wwsAAAAAnOHSW/UGDhyo6dOnKzo6Wnv37tWAAQOUlJRkv/VuyJAhCg8Pt/dv166dFi9erKlTp2rfvn369ttvFRkZqbp166pEiRKu2gwAAAAA/3Burlx5ly5ddPLkSY0aNUrJycmqWrWqVqxYoaCgIElScnKyw2869ejRQ2fPntVHH32kQYMGqUCBAnrkkUf07rvvumoTAAAAANwDXPaMk6s4cx8jAAAAgH+uv8UzTgAAAADwd0FwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALBCcAAAAAsEBwAgAAAAALbq4u4J8srMsoV5cA/C2tjh3m6hJyVM0xI1xdAvC3k/DWCFeXAAAOuOIEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABb4HScAAIA77OU1/V1dAvC3NLnZB64uwY4rTgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABggeAEAAAAABYITgAAAABg4baC08WLF3OqDgAAAADItZwOTunp6Ro9erRKliwpPz8/7du3T5L09ttva8aMGTleIAAAAAC4mtPBacyYMYqJidF7770nDw8Pe3u1atU0ffp0pwuYMmWKQkJC5OXlpdDQUG3cuPGm/VNTUzV06FAFBQXJ09NT5cqVU3R0tNPrBQAAAIDscjo4zZ49W59++qmeeeYZ5c2b195evXp1/fTTT04tKzY2Vq+88oqGDh2qHTt2qFGjRmrVqpWSkpKynKdz585as2aNZsyYoZ9//lnz5s1TpUqVnN0MAAAAAMg2N2dnOHz4sMqXL5+hPT09XZcvX3ZqWRMnTlSvXr3Uu3dvSdKkSZO0atUqTZ06VVFRURn6r1y5UvHx8dq3b58KFSokSQoODnZ2EwAAAADAKU5fcbr//vszvZ1u4cKFeuCBB7K9nEuXLmn79u0KCwtzaA8LC9PmzZsznefLL79U7dq19d5776lkyZKqUKGCXn31VV24cCHL9aSmpurMmTMOLwAAAABwhtNXnIYPH65u3brp8OHDSk9P1+LFi/Xzzz9r9uzZWr58ebaXc+LECaWlpSkwMNChPTAwUEePHs10nn379mnTpk3y8vLSkiVLdOLECfXr10+nTp3K8jmnqKgojRw5MvsbCAAAAAA3cPqKU7t27RQbG6sVK1bIZrNp2LBh2rt3r5YtW6bmzZs7XYDNZnN4b4zJ0HZNenq6bDab5s6dq7p166p169aaOHGiYmJisrzqNGTIEKWkpNhfBw8edLpGAAAAAPc2p644XblyRWPHjlVERITi4+Nva8WFCxdW3rx5M1xdOn78eIarUNcUL15cJUuWlL+/v72tcuXKMsbo0KFDuu+++zLM4+npKU9Pz9uqFQAAAMC9zakrTm5ubho/frzS0tJue8UeHh4KDQ1VXFycQ3tcXJwaNmyY6TwPPvigjhw5onPnztnb/ve//ylPnjwqVarUbdcEAAAAAJlx+la9Rx99VOvXr8+RlQ8cOFDTp09XdHS09u7dqwEDBigpKUl9+/aVdPU2u/DwcHv/rl27KiAgQD179lRiYqI2bNig1157TREREfL29s6RmgAAAADgRk4PDtGqVSsNGTJEu3fvVmhoqHx9fR2mP/bYY9leVpcuXXTy5EmNGjVKycnJqlq1qlasWKGgoCBJUnJyssNvOvn5+SkuLk4vv/yyateurYCAAHXu3FljxoxxdjMAAAAAINucDk4vvPCCpKu/wXQjm83m9G18/fr1U79+/TKdFhMTk6GtUqVKGW7vAwAAAIA7yenglJ6efifqAAAAAIBcy+lnnAAAAADgXnNLwSk+Pl7t2rVT+fLldd999+mxxx7Txo0bc7o2AAAAAMgVnA5On332mR599FH5+PgoMjJSL730kry9vdWsWTN9/vnnd6JGAAAAAHApp59xGjt2rN577z0NGDDA3ta/f39NnDhRo0ePVteuXXO0QAAAAABwNaevOO3bt0/t2rXL0P7YY49p//79OVIUAAAAAOQmTgen0qVLa82aNRna16xZo9KlS+dIUQAAAACQmzh9q96gQYMUGRmphIQENWzYUDabTZs2bVJMTIw++OCDO1EjAAAAALjULf0AbrFixTRhwgQtWLBAklS5cmXFxsaqffv2OV4gAAAAALia08FJkjp27KiOHTvmdC0AAAAAkCs5/YzT999/r61bt2Zo37p1q7Zt25YjRQEAAABAbuJ0cHrxxRd18ODBDO2HDx/Wiy++mCNFAQAAAEBu4nRwSkxMVK1atTK0P/DAA0pMTMyRogAAAAAgN3E6OHl6eurYsWMZ2pOTk+XmdkuPTAEAAABAruZ0cGrevLmGDBmilJQUe9vp06f15ptvqnnz5jlaHAAAAADkBk5fIpowYYIaN26soKAgPfDAA5KkhIQEBQYGas6cOTleIAAAAAC4mtPBqWTJktq5c6fmzp2rH3/8Ud7e3urZs6eefvppubu734kaAQAAAMClbumhJF9fX/Xp0yenawEAAACAXCnbzzj9+uuv2r59u0PbmjVr1LRpU9WtW1fvvPNOjhcHAAAAALlBtoPTa6+9pqVLl9rf79+/X+3atZOHh4caNGigqKgoTZo06Q6UCAAAAACule1b9bZt26bBgwfb38+dO1cVKlTQqlWrJEnVq1fX5MmT9corr+R4kQAAAADgStm+4nTixAmVKlXK/n7dunVq166d/f3DDz+s33//PUeLAwAAAIDcINvBqVChQkpOTpYkpaena9u2bapXr559+qVLl2SMyfkKAQAAAMDFsh2cmjRpotGjR+vgwYOaNGmS0tPT1bRpU/v0xMREBQcH34kaAQAAAMClsv2M09ixY9W8eXMFBwcrT548+vDDD+Xr62ufPmfOHD3yyCN3pEgAAAAAcKVsB6eQkBDt3btXiYmJKlKkiEqUKOEwfeTIkQ7PQAEAAADAP4VTP4Dr7u6uGjVqZDotq3YAAAAA+LvL9jNOAAAAAHCvIjgBAAAAgAWCEwAAAABYIDgBAAAAgAWng1NwcLBGjRqlpKSkO1EPAAAAAOQ6TgenQYMG6T//+Y/Kli2r5s2ba/78+UpNTb0TtQEAAABAruB0cHr55Ze1fft2bd++XVWqVFFkZKSKFy+ul156ST/88MOdqBEAAAAAXOqWn3GqUaOGPvjgAx0+fFjDhw/X9OnTVadOHdWoUUPR0dEyxuRknQAAAADgMk79AO71Ll++rCVLlmjmzJmKi4tT/fr11atXLx05ckRDhw7VN998o88//zwnawUAAAAAl3A6OP3www+aOXOm5s2bp7x586pbt27617/+pUqVKtn7hIWFqXHjxjlaKAAAAAC4itPBqU6dOmrevLmmTp2qDh06yN3dPUOfKlWq6KmnnsqRAgEAAADA1ZwOTvv27VNQUNBN+/j6+mrmzJm3XBQAAAAA5CZODw5x/Phxbd26NUP71q1btW3bthwpCgAAAAByE6eD04svvqiDBw9maD98+LBefPHFHCkKAAAAAHITp4NTYmKiatWqlaH9gQceUGJiYo4UBQAAAAC5idPBydPTU8eOHcvQnpycLDe3Wx7dHAAAAAByLaeDU/PmzTVkyBClpKTY206fPq0333xTzZs3z9HiAAAAACA3cPoS0YQJE9S4cWMFBQXpgQcekCQlJCQoMDBQc+bMyfECAQAAAMDVnA5OJUuW1M6dOzV37lz9+OOP8vb2Vs+ePfX0009n+ptOAAAAAPB3d0sPJfn6+qpPnz45XQsAAAAA5Eq3PJpDYmKikpKSdOnSJYf2xx577LaLAgAAAIDcxOngtG/fPnXs2FG7du2SzWaTMUaSZLPZJElpaWk5WyEAAAAAuJjTo+r1799fISEhOnbsmHx8fLRnzx5t2LBBtWvX1vr16+9AiQAAAADgWk5fcdqyZYvWrl2rIkWKKE+ePMqTJ48eeughRUVFKTIyUjt27LgTdQIAAACAyzh9xSktLU1+fn6SpMKFC+vIkSOSpKCgIP388885Wx0AAAAA5AJOX3GqWrWqdu7cqbJly6pevXp677335OHhoU8//VRly5a9EzUCAAAAgEs5HZzeeustnT9/XpI0ZswYtW3bVo0aNVJAQIBiY2NzvEAAAAAAcDWng1OLFi3s/79s2bJKTEzUqVOnVLBgQfvIegAAAADwT+LUM05XrlyRm5ubdu/e7dBeqFAhQhMAAACAfyyngpObm5uCgoL4rSYAAAAA9xSnR9V76623NGTIEJ06depO1AMAAAAAuY7Tzzh9+OGH+vXXX1WiRAkFBQXJ19fXYfoPP/yQY8UBAAAAQG7gdHDq0KHDHSgDAAAAAHIvp4PT8OHD70QdAAAAAJBrOf2MEwAAAADca5y+4pQnT56bDj3OiHsAAAAA/mmcDk5LlixxeH/58mXt2LFDs2bN0siRI3OsMAAAAADILZwOTu3bt8/Q1qlTJ91///2KjY1Vr169cqQwAAAAAMgtcuwZp3r16umbb77JqcUBAAAAQK6RI8HpwoULmjx5skqVKpUTiwMAAACAXMXpW/UKFizoMDiEMUZnz56Vj4+PPvvssxwtDgAAAAByA6eD07/+9S+H4JQnTx4VKVJE9erVU8GCBXO0OAAAAADIDZwOTj169LgDZQAAAABA7uX0M04zZ87UwoULM7QvXLhQs2bNypGiAAAAACA3cTo4jRs3ToULF87QXrRoUb3zzjs5UhQAAAAA5CZOB6cDBw4oJCQkQ3tQUJCSkpJypCgAAAAAyE2cDk5FixbVzp07M7T/+OOPCggIyJGiAAAAACA3cTo4PfXUU4qMjNS6deuUlpamtLQ0rV27Vv3799dTTz11J2oEAAAAAJdyelS9MWPG6MCBA2rWrJnc3K7Onp6ervDwcJ5xAgAAAPCP5HRw8vDwUGxsrMaMGaOEhAR5e3urWrVqCgoKuhP1AQAAAIDLOR2crrnvvvt033335WQtAAAAAJArOf2MU6dOnTRu3LgM7ePHj9eTTz6ZI0UBAAAAQG7idHCKj49XmzZtMrS3bNlSGzZsyJGiAAAAACA3cTo4nTt3Th4eHhna3d3ddebMmRwpCgAAAAByE6eDU9WqVRUbG5uhff78+apSpYrTBUyZMkUhISHy8vJSaGioNm7cmK35vv32W7m5ualmzZpOrxMAAAAAnOH04BBvv/22nnjiCf3222965JFHJElr1qzRvHnztHDhQqeWFRsbq1deeUVTpkzRgw8+qE8++UStWrVSYmKiypQpk+V8KSkpCg8PV7NmzXTs2DFnNwEAAAAAnOL0FafHHntMS5cu1a+//qp+/fpp0KBBOnTokL755ht16NDBqWVNnDhRvXr1Uu/evVW5cmVNmjRJpUuX1tSpU2863/PPP6+uXbuqQYMGzpYPAAAAAE5zOjhJUps2bfTtt9/q/PnzOnHihNauXasmTZooISEh28u4dOmStm/frrCwMIf2sLAwbd68Ocv5Zs6cqd9++03Dhw/P1npSU1N15swZhxcAAAAAOOOWgtP1UlJSNGXKFNWqVUuhoaHZnu/EiRNKS0tTYGCgQ3tgYKCOHj2a6Ty//PKL3njjDc2dO1dubtm7yzAqKkr+/v72V+nSpbNdIwAAAABItxGc1q5dq2eeeUbFixfX5MmT1bp1a23bts3p5dhsNof3xpgMbZKUlpamrl27auTIkapQoUK2lz9kyBClpKTYXwcPHnS6RgAAAAD3NqcGhzh06JBiYmIUHR2t8+fPq3Pnzrp8+bIWLVrk9Ih6hQsXVt68eTNcXTp+/HiGq1CSdPbsWW3btk07duzQSy+9JElKT0+XMUZubm5avXq1fbCK63l6esrT09Op2gAAAADgetm+4tS6dWtVqVJFiYmJmjx5so4cOaLJkyff8oo9PDwUGhqquLg4h/a4uDg1bNgwQ//8+fNr165dSkhIsL/69u2rihUrKiEhQfXq1bvlWgAAAADgZrJ9xWn16tWKjIzUCy+8oPvuuy9HVj5w4EB169ZNtWvXVoMGDfTpp58qKSlJffv2lXT1NrvDhw9r9uzZypMnj6pWreowf9GiReXl5ZWhHQAAAAByUraD08aNGxUdHa3atWurUqVK6tatm7p06XJbK+/SpYtOnjypUaNGKTk5WVWrVtWKFSsUFBQkSUpOTlZSUtJtrQMAAAAAble2b9Vr0KCB/v3vfys5OVnPP/+85s+fr5IlSyo9PV1xcXE6e/bsLRXQr18//f7770pNTdX27dvVuHFj+7SYmBitX78+y3lHjBjh1BDoAAAAAHArnB5Vz8fHRxEREdq0aZN27dqlQYMGady4cSpatKgee+yxO1EjAAAAALjUbf2OU8WKFfXee+/p0KFDmjdvXk7VBAAAAAC5ym3/AK4k5c2bVx06dNCXX36ZE4sDAAAAgFwlR4ITAAAAAPyTEZwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwALBCQAAAAAsEJwAAAAAwILLg9OUKVMUEhIiLy8vhYaGauPGjVn2Xbx4sZo3b64iRYoof/78atCggVatWnUXqwUAAABwL3JpcIqNjdUrr7yioUOHaseOHWrUqJFatWqlpKSkTPtv2LBBzZs314oVK7R9+3Y1bdpU7dq1044dO+5y5QAAAADuJS4NThMnTlSvXr3Uu3dvVa5cWZMmTVLp0qU1derUTPtPmjRJgwcPVp06dXTffffpnXfe0X333adly5bd5coBAAAA3EtcFpwuXbqk7du3KywszKE9LCxMmzdvztYy0tPTdfbsWRUqVOhOlAgAAAAAkiQ3V634xIkTSktLU2BgoEN7YGCgjh49mq1lTJgwQefPn1fnzp2z7JOamqrU1FT7+zNnztxawQAAAADuWS4fHMJmszm8N8ZkaMvMvHnzNGLECMXGxqpo0aJZ9ouKipK/v7/9Vbp06duuGQAAAMC9xWXBqXDhwsqbN2+Gq0vHjx/PcBXqRrGxserVq5cWLFigRx999KZ9hwwZopSUFPvr4MGDt107AAAAgHuLy4KTh4eHQkNDFRcX59AeFxenhg0bZjnfvHnz1KNHD33++edq06aN5Xo8PT2VP39+hxcAAAAAOMNlzzhJ0sCBA9WtWzfVrl1bDRo00KeffqqkpCT17dtX0tWrRYcPH9bs2bMlXQ1N4eHh+uCDD1S/fn371Spvb2/5+/u7bDsAAAAA/LO5NDh16dJFJ0+e1KhRo5ScnKyqVatqxYoVCgoKkiQlJyc7/KbTJ598oitXrujFF1/Uiy++aG/v3r27YmJi7nb5AAAAAO4RLg1OktSvXz/169cv02k3hqH169ff+YIAAAAA4AYuH1UPAAAAAHI7ghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWCA4AQAAAIAFghMAAAAAWHB5cJoyZYpCQkLk5eWl0NBQbdy48ab94+PjFRoaKi8vL5UtW1bTpk27S5UCAAAAuFe5NDjFxsbqlVde0dChQ7Vjxw41atRIrVq1UlJSUqb99+/fr9atW6tRo0basWOH3nzzTUVGRmrRokV3uXIAAAAA9xKXBqeJEyeqV69e6t27typXrqxJkyapdOnSmjp1aqb9p02bpjJlymjSpEmqXLmyevfurYiICL3//vt3uXIAAAAA9xI3V6340qVL2r59u9544w2H9rCwMG3evDnTebZs2aKwsDCHthYtWmjGjBm6fPmy3N3dM8yTmpqq1NRU+/uUlBRJ0pkzZ253EyxduXzxjq8D+Ce6G5/PuyntYqp1JwAO/mnngUvnOQ8At+JOnwuuLd8YY9nXZcHpxIkTSktLU2BgoEN7YGCgjh49muk8R48ezbT/lStXdOLECRUvXjzDPFFRURo5cmSG9tKlS99G9QDuJP8lUa4uAYCL+Y8d5+oSAOQCn+qTu7Kes2fPyt/f/6Z9XBacrrHZbA7vjTEZ2qz6Z9Z+zZAhQzRw4ED7+/T0dJ06dUoBAQE3XQ/+2c6cOaPSpUvr4MGDyp8/v6vLAeACnAcAcB6AMUZnz55ViRIlLPu6LDgVLlxYefPmzXB16fjx4xmuKl1TrFixTPu7ubkpICAg03k8PT3l6enp0FagQIFbLxz/KPnz5+dECdzjOA8A4Dxwb7O60nSNywaH8PDwUGhoqOLi4hza4+Li1LBhw0znadCgQYb+q1evVu3atTN9vgkAAAAAcoJLR9UbOHCgpk+frujoaO3du1cDBgxQUlKS+vbtK+nqbXbh4eH2/n379tWBAwc0cOBA7d27V9HR0ZoxY4ZeffVVV20CAAAAgHuAS59x6tKli06ePKlRo0YpOTlZVatW1YoVKxQUFCRJSk5OdvhNp5CQEK1YsUIDBgzQxx9/rBIlSujDDz/UE0884apNwN+Up6enhg8fnuE2TgD3Ds4DADgPwBk2k52x9wAAAADgHubSW/UAAAAA4O+A4AQAAAAAFghOAAAAAGCB4IS7xmazaenSpa4uw6V69OihDh06ZLv/+vXrZbPZdPr06TtWE+AqNx7fMTEx/M6enD9XOnteAXIbzgWZ41yQ+xCckGOsPrDJyclq1arV3SvISTabTTabTd99951De2pqqgICAmSz2bR+/XrXFAfcAT169JDNZtO4ceMc2pcuXSqbzXbX6+nSpYv+97//3fX1ZldMTIxsNpsqV66cYdqCBQtks9kUHBx89wuzcPHiRfXo0UPVqlWTm5sbX6yQAecC5/xdzwXr169X+/btVbx4cfn6+qpmzZqaO3euq8v6WyE44a4pVqyYy4f7NMboypUrWU4vXbq0Zs6c6dC2ZMkS+fn53enSAJfw8vLSu+++qz///NPVpcjb21tFixZ1dRm6dOlSltN8fX11/PhxbdmyxaE9OjpaZcqUudOl3ZK0tDR5e3srMjJSjz76qKvLQS7FuSCjf9q5YPPmzapevboWLVqknTt3KiIiQuHh4Vq2bJmrS/vbIDjhrrn+kvPvv/8um82mxYsXq2nTpvLx8VGNGjUynIA2b96sxo0by9vbW6VLl1ZkZKTOnz9vn/7ZZ5+pdu3aypcvn4oVK6auXbvq+PHj9unXLv+vWrVKtWvXlqenpzZu3Jhljd27d9f8+fN14cIFe1t0dLS6d++eoe+uXbv0yCOPyNvbWwEBAerTp4/OnTtnn56WlqaBAweqQIECCggI0ODBg3Xj6P/GGL333nsqW7asvL29VaNGDX3xxRfZ26FADnj00UdVrFgxRUVF3bTfokWLdP/998vT01PBwcGaMGGCw/Tg4GC98847ioiIUL58+VSmTBl9+umnTtVy4+05I0aMUM2aNTVnzhwFBwfL399fTz31lM6ePWvvY/UZSktLU69evRQSEiJvb29VrFhRH3zwgcN6r10tj4qKUokSJVShQoUsa3Rzc1PXrl0VHR1tbzt06JDWr1+vrl27Zug/depUlStXTh4eHqpYsaLmzJnjMP2XX35R48aN5eXlpSpVqiguLi7DMg4fPqwuXbqoYMGCCggIUPv27fX7779nWeONfH19NXXqVD333HMqVqxYtufDvYVzwT//XPDmm29q9OjRatiwocqVK6fIyEi1bNlSS5YsyfYy7nUEJ7jU0KFD9eqrryohIUEVKlTQ008/bb8itGvXLrVo0UKPP/64du7cqdjYWG3atEkvvfSSff5Lly5p9OjR+vHHH7V06VLt379fPXr0yLCewYMHKyoqSnv37lX16tWzrCc0NFQhISFatGiRJOngwYPasGGDunXr5tDvr7/+UsuWLVWwYEF9//33Wrhwob755huH2iZMmKDo6GjNmDFDmzZt0qlTpzKcnN566y3NnDlTU6dO1Z49ezRgwAA9++yzio+Pd3pfArcib968eueddzR58mQdOnQo0z7bt29X586d9dRTT2nXrl0aMWKE3n77bcXExDj0mzBhgmrXrq0dO3aoX79+euGFF/TTTz/dVn2//fabli5dquXLl2v58uWKj493uJ3I6jOUnp6uUqVKacGCBUpMTNSwYcP05ptvasGCBQ7rWbNmjfbu3au4uDgtX778pjX16tVLsbGx+uuvvyRd/ZLXsmVLBQYGOvRbsmSJ+vfvr0GDBmn37t16/vnn1bNnT61bt85e2+OPP668efPqu+++07Rp0/T66687LOOvv/5S06ZN5efnpw0bNmjTpk3y8/NTy5Ytb/qv4YCzOBdcda+dC1JSUlSoUKFbnv+eY4Ac0r17d9O+ffssp0syS5YsMcYYs3//fiPJTJ8+3T59z549RpLZu3evMcaYbt26mT59+jgsY+PGjSZPnjzmwoULma7jv//9r5Fkzp49a4wxZt26dUaSWbp0qWX91+qbNGmSadq0qTHGmJEjR5qOHTuaP//800gy69atM8YY8+mnn5qCBQuac+fO2ef/6quvTJ48eczRo0eNMcYUL17cjBs3zj798uXLplSpUvZ9dO7cOePl5WU2b97sUEevXr3M008/7VD/n3/+aVk/4KzrP7P169c3ERERxhhjlixZYq7/z0PXrl1N8+bNHeZ97bXXTJUqVezvg4KCzLPPPmt/n56ebooWLWqmTp2a5fpvPL5nzpxp/P397dOHDx9ufHx8zJkzZxzWW69ePWNM9j5DmenXr5954oknHPZDYGCgSU1NzXKeG+urWbOmmTVrlklPTzflypUz//nPf8y//vUvExQUZO/fsGFD89xzzzks48knnzStW7c2xhizatUqkzdvXnPw4EH79K+//trhXDljxgxTsWJFk56ebu+TmppqvL29zapVq+z13+zcez1n+uLewbng//fDvXIuMMaYhQsXGg8PD7N79+5sz3Ov44oTXOr6qz/FixeXJPutdtu3b1dMTIz8/PzsrxYtWig9PV379++XJO3YsUPt27dXUFCQ8uXLp4cffliSlJSU5LCe2rVrZ7umZ599Vlu2bNG+ffsUExOjiIiIDH327t2rGjVqyNfX19724IMPKj09XT///LNSUlKUnJysBg0a2Ke7ubk51JGYmKiLFy+qefPmDts4e/Zs/fbbb9muF8gJ7777rmbNmqXExMQM0/bu3asHH3zQoe3BBx/UL7/8orS0NHvb9Z9nm82mYsWK2T/PrVq1sh/j999/f7brCg4OVr58+ezvixcvbl9mdj9D06ZNU+3atVWkSBH5+fnp3//+d4ZzRLVq1eTh4ZHtuiIiIjRz5kzFx8fr3Llzat26dYY+We23vXv32qeXKVNGpUqVsk+//pwhXT0P/vrrr8qXL599+woVKqSLFy9ynsAdwbng3jgXrF+/Xj169NC///1vp/4O9zo3VxeAe5u7u7v9/18buSc9Pd3+v88//7wiIyMzzFemTBmdP39eYWFhCgsL02effaYiRYooKSlJLVq0yHDZ+vqAYyUgIEBt27ZVr169dPHiRbVq1crhPmrp6r3UWY00lN0RiK5t51dffaWSJUs6THP1IBq49zRu3FgtWrTQm2++meF218yOd3PD83qS4+dZuvpZuHacT58+3f7s4I39buZmy8zOZ2jBggUaMGCAJkyYoAYNGihfvnwaP368tm7d6tDfmXOEJD3zzDMaPHiwRowYofDwcLm5Zf6f08z227W2zPbhjf3T09MVGhqa6chXRYoUcapmIDs4F/zzzwXx8fFq166dJk6cqPDwcKfmvdcRnJBr1apVS3v27FH58uUznb5r1y6dOHFC48aNU+nSpSVJ27Zty5F1R0REqHXr1nr99deVN2/eDNOrVKmiWbNm6fz58/aT7Lfffqs8efKoQoUK8vf3V/HixfXdd9+pcePGkqQrV65o+/btqlWrln0Znp6eSkpKUpMmTXKkbuB2jBs3TjVr1szwQHSVKlW0adMmh7bNmzerQoUKmX4+MnPjl5mckJ3P0MaNG9WwYUP169fP3pYTV2oKFSqkxx57TAsWLNC0adMy7VO5cmVt2rTJ4YvJ5s2b7UMYV6lSRUlJSTpy5IhKlCghSRkGyKlVq5ZiY2NVtGhR5c+f/7brBrKDc0H2/d3OBevXr1fbtm317rvvqk+fPre8nHsVwQk5KiUlRQkJCQ5thQoVuqWhOV9//XXVr19fL774op577jn5+vraH9icPHmyypQpIw8PD02ePFl9+/bV7t27NXr06BzZjpYtW+qPP/7I8uT0zDPPaPjw4erevbtGjBihP/74Qy+//LK6detmfyi0f//+GjdunO677z5VrlxZEydOdPgh23z58unVV1/VgAEDlJ6eroceekhnzpzR5s2b5efnl+lIfsCdVK1aNT3zzDOaPHmyQ/ugQYNUp04djR49Wl26dNGWLVv00UcfacqUKS6q9KrsfIbKly+v2bNna9WqVQoJCdGcOXP0/fffKyQk5LbXHxMToylTpiggICDT6a+99po6d+6sWrVqqVmzZlq2bJkWL16sb775RtLVUcwqVqyo8PBwTZgwQWfOnNHQoUMdlvHMM89o/Pjxat++vUaNGqVSpUopKSlJixcv1muvveZwa8/NJCYm6tKlSzp16pTOnj1rP0/XrFnzlrcf/1ycC5zzdzkXrF+/Xm3atFH//v31xBNP6OjRo5IkDw8PBojIJp5xQo5av369HnjgAYfXsGHDbmlZ1atXV3x8vH755Rc1atRIDzzwgN5++237s1BFihRRTEyMFi5cqCpVqmjcuHF6//33c2Q7bDabChcunOV9zj4+Plq1apVOnTqlOnXqqFOnTmrWrJk++ugje59BgwYpPDxcPXr0sN8W0LFjR4fljB49WsOGDVNUVJQqV66sFi1aaNmyZTlyIgduxejRozPcNlKrVi0tWLBA8+fPV9WqVTVs2DCNGjUq0xEs7zarz1Dfvn31+OOPq0uXLqpXr55Onjzp8C/Ot+PaTxFkpUOHDvrggw80fvx43X///frkk080c+ZM+7OYefLk0ZIlS5Samqq6deuqd+/eGjt2rMMyfHx8tGHDBpUpU0aPP/64KleurIiICF24cMGpf3Vu3bq1HnjgAS1btszhPA1khXNB9v1dzgUxMTH666+/FBUVpeLFi9tfjz/++C1v+73GZjK7sRIAAAAAYMcVJwAAAACwQHACAAAAAAsEJwAAAACwQHACAAAAAAsEJwAAAACwQHACAAAAAAsEJwAAAACwQHACAAAAAAsEJwAAAACwQHACAAAAAAsEJwAAAACwQHACAAAAAAv/B2JDl4VIemwvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "\n",
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Fetch dataset\n",
    "heart_failure_clinical_records = fetch_ucirepo(id=519)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = heart_failure_clinical_records.data.features\n",
    "y = heart_failure_clinical_records.data.targets.values.ravel()\n",
    "\n",
    "# Ensure the number of samples in X and y is consistent\n",
    "if X.shape[0] != y.shape[0]:\n",
    "    raise ValueError(\"Number of samples in X and y are inconsistent.\")\n",
    "\n",
    "# Split the data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define preprocessing steps using ColumnTransformer\n",
    "numeric_features = ['age', 'serum_creatinine']\n",
    "categorical_features = ['sex']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n",
    "    ('scaler', StandardScaler())  # Standardize numeric features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Replace missing values with most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define machine learning models\n",
    "linear_model = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', penalty='l1')\n",
    "nonlinear_model1 = RandomForestClassifier(random_state=42)\n",
    "nonlinear_model2 = SVC(random_state=42)\n",
    "\n",
    "# Define the pipeline for each model\n",
    "linear_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', linear_model)\n",
    "])\n",
    "\n",
    "nonlinear_pipeline1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', nonlinear_model1)\n",
    "])\n",
    "\n",
    "nonlinear_pipeline2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', nonlinear_model2)\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search for each model\n",
    "param_grid_linear = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "param_grid_nonlinear1 = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "param_grid_nonlinear2 = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "grid_search_linear = GridSearchCV(linear_pipeline, param_grid_linear, cv=5, scoring='accuracy', error_score='raise')\n",
    "grid_search_linear.fit(X_train, y_train)\n",
    "\n",
    "grid_search_nonlinear1 = GridSearchCV(nonlinear_pipeline1, param_grid_nonlinear1, cv=5, scoring='accuracy', error_score='raise')\n",
    "grid_search_nonlinear1.fit(X_train, y_train)\n",
    "\n",
    "grid_search_nonlinear2 = GridSearchCV(nonlinear_pipeline2, param_grid_nonlinear2, cv=5, scoring='accuracy', error_score='raise')\n",
    "grid_search_nonlinear2.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters for each model\n",
    "print(\"Best Hyperparameters for Linear Model:\", grid_search_linear.best_params_)\n",
    "print(\"Best Hyperparameters for Non-linear Model 1:\", grid_search_nonlinear1.best_params_)\n",
    "print(\"Best Hyperparameters for Non-linear Model 2:\", grid_search_nonlinear2.best_params_)\n",
    "\n",
    "# Evaluate each model on the test set\n",
    "y_pred_linear = grid_search_linear.predict(X_test)\n",
    "y_pred_nonlinear1 = grid_search_nonlinear1.predict(X_test)\n",
    "y_pred_nonlinear2 = grid_search_nonlinear2.predict(X_test)\n",
    "# Display evaluation metrics for each model\n",
    "print(\"\\nAccuracy on Test Set (Linear Model):\", accuracy_score(y_test, y_pred_linear))\n",
    "print(\"Classification Report (Linear Model):\\n\", classification_report(y_test, y_pred_linear))\n",
    "\n",
    "print(\"\\nAccuracy on Test Set (Non-linear Model 1):\", accuracy_score(y_test, y_pred_nonlinear1))\n",
    "print(\"Classification Report (Non-linear Model 1):\\n\", classification_report(y_test, y_pred_nonlinear1))\n",
    "\n",
    "print(\"\\nAccuracy on Test Set (Non-linear Model 2):\", accuracy_score(y_test, y_pred_nonlinear2))\n",
    "print(\"Classification Report (Non-linear Model 2):\\n\", classification_report(y_test, y_pred_nonlinear2))\n",
    "\n",
    "# Accuracy scores for each model\n",
    "accuracy_scores = [accuracy_score(y_test, y_pred_linear),\n",
    "                   accuracy_score(y_test, y_pred_nonlinear1),\n",
    "                   accuracy_score(y_test, y_pred_nonlinear2)]\n",
    "\n",
    "# Model names\n",
    "model_names = ['Linear Model', 'Non-linear Model 1', 'Non-linear Model 2']\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=model_names, y=accuracy_scores, palette='viridis')\n",
    "plt.title('Accuracy on Test Set for Different Models')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.ylim(0, 1)  # Set the y-axis limit between 0 and 1 for accuracy score\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "2. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "3. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. Which accuracy metric did you choose? \n",
    "The accuracy score was used as the evaluation metric. The accuracy score is a commonly used metric for classification tasks and represents the ratio of correctly predicted instances to the total number of instances in the test set.\n",
    "\n",
    "2. How do these results compare to those in part 3? Did this model generalize well?\n",
    "In part 3, the testing set exhibited a consistent performance with the training set in part 5 with an output of the same values.\n",
    "Since there is a consistent performance between the testing set and the training set, it implies that the model generalized well. (Accuracy, precision, recall, and F1-score) obtained in part 3 with the results in part 5 are consistent and are the same which suggests good generalization.\n",
    "\n",
    "3. Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "Performance Evaluation:\n",
    "\n",
    "Linear Model: The linear model achieved an accuracy of 70% on the test set. The classification report provides detailed metrics for precision, recall, and F1-score.\n",
    "Non-linear Model 1 (Random Forest): This model also achieved a 70% accuracy on the test set, with precision, recall, and F1-score metrics available.\n",
    "Non-linear Model 2 (SVM): The accuracy of this model was approximately 68.3% on the test set, with precision, recall, and F1-score metrics.\n",
    "\n",
    "Comparison with Training Set:\n",
    "The accuracy scores obtained on the test set are somewhat comparable to the cross-validated results during grid search, suggesting a reasonable level of generalization.\n",
    "\n",
    "Real-world Applicability:\n",
    "\n",
    "Yes the Random Forest classifier model which is my best model performed \"well enough\" to be used out in the real-world because it has a 70% accuracy with a 74% precision.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "2. In what order did you complete the steps?\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "1. Where did you source your code?\n",
    "I sourced my code from the lecture notes, execises and examples given and also Google and Generative AI and StackOverflow\n",
    "List of References\n",
    "OpenAI. (2023). [Website]. https://www.openai.com\n",
    "Google. (2023). [Website]. https://www.google.com\n",
    "Stackoverflow. (2023).[Website]. https://stackoverflow.com/\n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "I completed the steps sequentially begining from Step 1,2,3 and 4\n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I prompted it in my words to generate ideas to write the code and understand the syntax behind the code. Yes I modifed the code many times because in most cases, it produced what I didn't need and in some cases, didn't include what I needed. So I modified it to add the missing part I needed and also the remove the unnecessary part which wasn't needed.\n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "Yes at some point, I encountered some errors in my code, especially in step 3 and also when I tried to visualize the code. I figured out the error and debugged and after which I was able to get an output and visualize my results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "What you liked:\n",
    "\n",
    "I liked the fact that I was able to explore a real-world dataset by working with a dataset related to heart failure clinical records which allows for the application of machine learning techniques to address a significant health-related problem.\n",
    "I also liked the practical experience in model selection and hyperparameter tuning offered while constructing machine learning pipelines and conducting grid searches \n",
    "In the same vein, I liked the fact that while analyzing and visualizing the results, an evaluated model performance and visualized outcome provided insights into the effectiveness of different algorithms.\n",
    "\n",
    "Nevertheless, I disliked dealing with missing values and it's challenges.\n",
    "\n",
    "What I found interesting:\n",
    "\n",
    "I find experimenting with different hyperparameter values during grid search quite interesting as it explores the impact of various configurations on model performance. Also I find interpreting the implications of the model performance on heart failure prediction to be intellectually stimulating.\n",
    "\n",
    "\n",
    "What I found challenging:\n",
    "Actually deciding on a particular dataset to use was quite challenging to me as I had to check for a meaningful dataset that had not been used in class or the labs and also that has a good source.\n",
    "\n",
    "\n",
    "What I found motivating:\n",
    "\n",
    "I was highly motivated when I got to know that the models developed could potentially contribute to predicting heart failure risk, thus aiding in early intervention in the real-world. Also I found it very motivating knowing that working on this assignment offered me the opportunity to apply machine learning techniques to a real-world health dataset, exploring challenges in data preprocessing, model selection, and hyperparameter tuning, with a focus on predicting heart failure risk.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
